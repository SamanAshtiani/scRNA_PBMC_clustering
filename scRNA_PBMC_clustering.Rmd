---
title: "Biostatistical Project"
author: "Saman"
date: "`r Sys.setlocale('LC_TIME', 'C');format(Sys.Date(),format='%B %e, %Y')`"
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: console
---


# Benchmark of full-length trnascript sc mRNA sequencing protocols
## SS3
Fourteen single cells were processed by SS3 protocol and sorted into 3 μl SS3 lysis buffer mix [35]. ScRNA-seq was performed as described by Sandberg et al. 2020. Each well was added a concentration of 1:106 ERCC spike ins (cat nr. 4,456,740, Invitrogen, Thermo Fischer Scientific, Lithuania). PCR mix was prepared using a working concentration of Kapa HiFi HotStart ReadyMix (1X) (Kapa, cat. no. KK2601), Fwrd. primer (0.5uM), Rev. primer (0.1uM). PCR amplification was performed applying 20 cycles.

## Sequencing
T47D single cell cDNA libraries were paired-end sequenced in groups according to each library preparation protocol (13 or 14 single cells per run) on MiSeq Benchtop Sequencer (Illumina, USA), using MiSeq Reagent kit v2 300 cycles (cat nr. MS-102–2002, Illumina, USA).

## Alignment/trimming

Illumina sequencing raw reads were converted to fastq files. Fastq files were processed on a bash shell. Raw reads were trimmed using Trim Galore v.0.4.0 [36] with default parameters, where two rounds of trimming were performed. The first trimming removed Nextera XT adaptors (”CTGTCTCTTATACACATCT”), and the second trimming removed cDNA amplification adaptors (”AAGCAGTGGTATCAACGCAGAGT”). Quality assessment of sequencing output was performed by two rounds of FastQC after each trim. Trimmed sequences were aligned using the splice-aware aligner STAR v2.5.2b [37] to Genome Reference Consortium Human Build 38 (GRCh38), with the following parameters: –genomeLoad NoSharedMemory –quantMode TranscriptomeSAM GeneCounts –readFilesCommand zcat –outSAMtype BAM SortedByCoordinate –limitBAMsortRAM 35,000,000,000. STAR output files comprising ReadsPerGene.out.tab for each sample, which were merged into a single tab delimited file, so called “count matrix”, for further analysis.

### another protocol for prcessing raw fastq reads:
For the analysis of mouse RNA expression, Illumina adapter sequences were first removed from the raw read (fastq) files using Cutadapt (Martin, 2011), and library quality was assessed with FastQC (https://www.bioinformatics.babraham.ac.uk/projects/fastqc/). Next, reads were aligned to the mm10 mouse genome using STAR (Dobin et al., 2013). Gene read counts were generated using Homer (Heinz et al., 2010) with a gtf reference file containing whole-gene annotations (thus counting both exonic and intronic reads). On average, ∼90% of mapped reads were uniquely mapped to the mm10 genome.

## Data visualization
ScRNA-seq data was imported into R studies as an expression matrix. The count matrix was transformed into a Single Cell Experiment Object (SCE-object) using Rstudios (v3.6.1 Opensource, https://rstudio.com/) package SingleCellExperiment (v1.6.0). Data graphs were generated using ggplot2 (v3.3.0). Plots are either Sinaplots [38] or box-plots where with outlier threshold at X and wiskers at Y, stars are denoted based on Wilcoxon test p-value; ns: p > 0.05, \*: p <  = 0.05, \**: p < 0.01, ***: p < 0.001 and ****: p < 0.0001.


## Single cell quality control
Expression level of genes were quantified by CPM (counts per million). Genes with an average expression above zero (CPM > 0) across all cells were kept in the dataset. Cells not expressed in any cell (CPM = 0) were filtered away. Bad quality cells (or empty wells) were filtered away based on the following criteria: 1) cells that had less than 1000 uniquely expressed genes, 2) cells that had library sizes below 0.6e6 million reads, 3) cells that had more than 30% reads mapped to mitochondrial genes, and 4) cells that had more than 25% of reads mapped to ERCC spike-inn genes. Data analysis was performed with RStudio (version 3.6.1) using Bioconductor [https://www.bioconductor.org] packages (SingleCellExperiment, sinaplot [38], scater [39], ggplot2, GenomicFeatures [40], sincell [41], TxDb.Hsapiens.UCSC.hg19.knownGene, SummarizedExperiment, robCompositions, splatter [42], reshape2, ggforce, gdata, hrbrthemes, viridis, VennDiagram, DESeq2 [43], dplyr, tidyverse [44], gtable, gridExtra, hrbrthemes, ggforce, ggpubr) following guidelines from [https://scrnaseq-course.cog.sanger.ac.uk/website/index.html].



# *** The roadmap1 ***
Step 1: Learn the basics of Smart-seq3
Before you begin working with Smart-seq3 raw reads, it's important to have a basic understanding of the technology and the sequencing process. You can start by reading the original Smart-seq3 paper by Hagemann-Jensen et al. (https://www.nature.com/articles/s41596-020-0413-2), which provides a detailed overview of the technology and its applications.

Step 2: Obtain the raw reads
The first step in processing Smart-seq3 data is to obtain the raw reads from your sequencing run. This will typically involve using a sequencing facility or service provider, such as a genomics core or a commercial sequencing company. Once you have obtained the raw reads, you will need to transfer them to your local computer or server for further analysis.

Step 3: Quality control and trimming
Before you begin aligning the reads, it's important to perform some basic quality control and trimming to remove low-quality reads and adaptors. The recommended tool for this step is fastp (https://github.com/OpenGene/fastp), which is a fast and efficient read pre-processing tool that can perform quality control, trimming, and filtering in a single step. You can install fastp using conda or pip:

conda install -c bioconda fastp
or

pip install fastp
To perform quality control and trimming with fastp, you can use the following command:

fastp -i <input.fastq> -o <output.fastq> -h <report.html> -j <report.json> -q 20 -u 50
This command will perform quality control with a minimum quality score of 20, trim adapters with a minimum overlap of 50 bases, and generate an HTML report and a JSON report file.

Step 4: Alignment
Once you have performed quality control and trimming, you can begin aligning the reads to a reference genome or transcriptome. The recommended tool for this step is STAR (https://github.com/alexdobin/STAR), which is a fast and accurate spliced read aligner that can align reads to both genomes and transcriptomes. You can install STAR using conda or download the binary from the STAR website:

conda install -c bioconda star
or

wget https://github.com/alexdobin/STAR/archive/2.7.9a.tar.gz
tar -xzf 2.7.9a.tar.gz
cd STAR-2.7.9a/source
make STAR
To align the reads with STAR, you will need a reference genome or transcriptome in FASTA and GTF format. You can download these files from a public database such as Ensembl or UCSC. Once you have downloaded the reference files, you can use the following command to align the reads:

STAR --genomeDir <path/to/genome/index> --readFilesIn <input.fastq> --outFileNamePrefix <output_prefix> --outSAMtype BAM SortedByCoordinate --quantMode GeneCounts --runThreadN <num_threads>
This command will align the reads to the reference genome, sort the resulting BAM file by coordinate, generate gene-level expression counts, and use multiple threads to speed up the alignment process.

Step 5: Quality control and downstream analysis
Once you have aligned the reads, you can perform some basic quality control and downstream analysis to validate the alignment and generate expression data. The recommended tools for this step are Picard (https://broadinstitute


# *** The roadmap 2 ***

Processing the raw reads from Smart-seq3:
a. The first step is to perform quality control checks on the raw reads. FastQC is a popular tool used for this purpose. You can download FastQC from the following URL: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/
b. Once you have FastQC installed, you can use it to assess the quality of the raw reads. The output generated by FastQC will help you to identify any potential issues with the raw reads, such as adapter contamination, low-quality reads, and over-represented sequences.
c. After quality control checks, you can perform pre-processing steps, such as filtering out low-quality reads and trimming adapter sequences. There are several tools available for this purpose, including Trimmomatic and Cutadapt. You can download Trimmomatic from the following URL: http://www.usadellab.org/cms/?page=trimmomatic and Cutadapt from the following URL: https://cutadapt.readthedocs.io/en/stable/
d. Once you have pre-processed the raw reads, you can proceed to the next step, which is alignment.

Trimming the raw reads from Smart-seq3:
a. Trimming is an essential step in the pre-processing of raw reads. It involves removing adapter sequences and low-quality bases from the ends of the reads. Trimming can improve the quality of the reads and increase the accuracy of downstream analyses.
b. Trimmomatic is a popular tool used for trimming raw reads. You can download Trimmomatic from the following URL: http://www.usadellab.org/cms/?page=trimmomatic
c. To use Trimmomatic, you need to provide it with the raw reads and specify the trimming parameters. Trimmomatic can trim both the 5' and 3' ends of the reads, remove adapter sequences, and filter out low-quality reads based on a user-defined quality threshold.
d. Once you have trimmed the raw reads, you can proceed to the next step, which is alignment.

Aligning the raw reads from Smart-seq3:
a. Alignment involves mapping the pre-processed reads to a reference genome. There are several alignment tools available, including STAR, HISAT2, and Bowtie2. You can download STAR from the following URL: https://github.com/alexdobin/STAR, HISAT2 from the following URL: http://daehwankimlab.github.io/hisat2/, and Bowtie2 from the following URL: http://bowtie-bio.sourceforge.net/bowtie2/index.shtml
b. To use these alignment tools, you need to provide them with the pre-processed reads and a reference genome. The alignment parameters can be adjusted based on the type of data and the research question.
c. After alignment, you can generate a BAM file, which contains the aligned reads in a compressed format. The BAM file can be further processed to generate gene expression counts and differential expression analyses.

d. Finally, you can use a tool called featureCounts to count the number of reads that map to each gene in the reference genome. featureCounts is a program that performs gene-level quantification of RNA-seq data. You can download featureCounts from the featureCounts website: http://subread.sourceforge.net/
Once you have installed featureCounts, you can use it to generate a count table that lists the number of reads that map to each gene in the reference genome.

Here are some additional resources that you may find helpful:

The Smart-seq3 protocol: https://protocols.io/view/smart-seq3-single-cell-rna-seq-for-the-human-p-arckcf6
The FastQC documentation: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/
The Trimmomatic documentation: http://www.usadellab.org/cms/?page=trimmomatic
The STAR documentation: https://github.com/alexdobin/STAR/blob/master/doc


# ***pipeline 1***
a. Quality control using FastQC

 Input: Raw reads in FASTQ format

 Output: HTML report containing information about the quality and composition of the reads

 ```
 fastqc input.fastq.gz
 ```
b. Trimming adapter sequences using Cutadapt

 Input: Raw reads in FASTQ format

 Output: Trimmed reads in FASTQ format

 ```
 cutadapt -a ADAPTER_SEQ -o output.fastq.gz input.fastq.gz
 ```

 Note: Replace `ADAPTER_SEQ` with the actual adapter sequence used in your experiment.
Aligning Trimmed Reads to a Reference Genome

a. Indexing the reference genome using STAR

 Input: Reference genome in FASTA format and annotation file in GTF format

 Output: STAR genome index directory containing indexed files for the reference genome and annotation

 ```
 STAR --runMode genomeGenerate --genomeDir /path/to/genomeDir --genomeFastaFiles /path/to/genome.fa --sjdbGTFfile /path/to/annotation.gtf --sjdbOverhang READ_LENGTH-1
 ```

 Note: Replace `/path/to/genomeDir`, `/path/to/genome.fa`, `/path/to/annotation.gtf`, and `READ_LENGTH` with the appropriate values for your reference genome and experiment.
b. Aligning the trimmed reads to the reference genome using STAR

 Input: Trimmed reads in FASTQ format and STAR genome index directory

 Output: Aligned reads in BAM format

 ```
 STAR --genomeDir /path/to/genomeDir --readFilesIn /path/to/trimmed_reads.fastq.gz --outFileNamePrefix output_prefix --outSAMtype BAM SortedByCoordinate --outSAMattributes NH HI AS NM MD
 ```

 Note: Replace `/path/to/genomeDir`, `/path/to/trimmed_reads.fastq.gz`, and `output_prefix` with the appropriate values for your experiment.
Generating Gene Expression Quantification

a. Counting the number of reads that map to each gene using featureCounts

 Input: Aligned reads in BAM format and annotation file in GTF format

 Output: Counts table in TXT format

 ```
 featureCounts -a /path/to/annotation.gtf -o counts.txt -t exon -g gene_id /path/to/aligned_reads.bam
 ```

 Note: Replace `/path/to/annotation.gtf`, `counts.txt`, and `/path/to/aligned_reads.bam` with the appropriate values for your experiment.
b. Normalizing the count table and performing differential expression analysis using DESeq2

 Input: Counts table in TXT format and sample metadata file in TXT format

 Output: Differential expression results table in TXT format

 ```
 library(DESeq2)
 countData <- as.matrix(read.table("counts.txt", header=TRUE, row.names=1))
 colData <- read.table("sample_metadata.txt", header=TRUE, row.names=1)
 dds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~condition)
 dds <- DESeq(dds)
 res <- results(dds)
 write.table(res, "differential_expression.txt", sep="\t", quote=FALSE)
 ```

 Note: Replace `counts.txt`, `sample_metadata.txt`, and `differential_ex
 
 
 Reference genome (in FASTA format):

You can download reference genomes from various sources depending on the organism you are working with. For example, you can download the human genome from the UCSC Genome Browser (https://hgdownload.soe.ucsc.edu/downloads.html) or the Ensembl website (https://www.ensembl.org/info/data/ftp/index.html).

Annotation file (in GTF format):

You can also download annotation files from various sources depending on the organism you are working with. For example, you can download the human annotation file from the Ensembl website (https://www.ensembl.org/info/data/ftp/index.html).

Metadata file (in tab-separated format):

The metadata file should contain information about the samples you are analyzing, such as the condition, replicate, and file name. You can create this file using a text editor and save it in a tab-separated format. Here is an example:

sql
Copy code
sample    condition    replicate    file
sample1   treatment    1            alignment1.bam
sample2   treatment    2            alignment2.bam
sample3   control      1            alignment3.bam
sample4   control      2            alignment4.bam
Genome index:

You can create a genome index using STAR. Here is an example command to generate the genome index for the human genome:

STAR --runMode genomeGenerate \
     --genomeDir /path/to/genome_index \
     --genomeFastaFiles /path/to/hg38.fa \
     --sjdbGTFfile /path/to/Homo_sapiens.GRCh38.102.gtf \
     --sjdbOverhang 100
Note that you will need to replace /path/to/genome_index, /path/to/hg38.fa, and /path/to/Homo_sapiens.GRCh38.102.gtf with the actual paths to the directories and files on your computer.

--runMode genomeGenerate: This flag specifies that we want to generate a genome index.

--genomeDir: This flag specifies the path to the directory where the genome index files will be saved.

--genomeFastaFiles: This flag specifies the path to the reference genome file in FASTA format.

--sjdbGTFfile: This flag specifies the path to the annotation file in GTF format.

--sjdbOverhang: This flag specifies the length of the genomic sequence that extends beyond the spliced junctions. The default value is 100, which is a good value for most cases. However, if you have longer RNA-seq reads, you may need to increase this value.

The STAR command for generating a genome index reads in the reference genome file and the annotation file, and builds a genome index that can be used for mapping reads to the reference genome. The genome index consists of a set of files that allow STAR to quickly and accurately align reads to the genome.

```{r seurat , include=TRUE,echo=TRUE, message=F}
setwd("~/git/scRNA_PBMC_clustering/")
install.packages('Seurat')
library(Seurat)


library(dplyr)
library(Seurat)
library(patchwork)

#load PBMC dataset which is the output of cellranger pipeline = a UMI count matrix. The values of this matrix represent the number of molecules detected for each feature in each cell
pbmc.data <- Read10X(data.dir = "./data/filtered_gene_bc_matrices/hg19/")
#initialize Seurat object with the raw (non-normalized) data
#min.cells:
#Include features detected in at least this many cells. Will subset the counts matrix as well. To reintroduce excluded features, create a new object with a lower cutoff
#min.features:Include cells where at least this many features are detected
pbmc <- CreateSeuratObject(counts = pbmc.data, project = "pbmc3k", min.cells = 3, min.features = 200)
pbmc
# look at the count matrix first few...
pbmc.data[c("CD3D", "TCL1A", "MS4A1"), 1:30]

#The . values in the matrix represent 0s (no molecules detected). Since most values in an scRNA-seq matrix are 0, Seurat uses a sparse-matrix representation whenever possible. This results in significant memory and speed savings for Drop-seq/inDrop/10x data.

dense.size <- object.size(as.matrix(pbmc.data))
dense.size
## 709591472 bytes
sparse.size <- object.size(pbmc.data)
sparse.size
## 29905192 bytes
dense.size/sparse.size
## 23.7 bytes

```

# Standard pre-processing workflow
Seurat allows you to easily explore QC metrics and filter cells based on any user-defined criteria. A few QC metrics commonly used by the community include:

* The number of unique genes detected in each cell.
  * Low-quality cells or empty droplets will often have very few genes
  * Cell doublets or multiplets may exhibit an aberrantly high gene count
* Similarly, the total number of molecules detected within a cell (correlates strongly with unique genes)
* The percentage of reads that map to the mitochondrial genome
  * Low-quality / dying cells often exhibit extensive mitochondrial contamination
  * We calculate mitochondrial QC metrics with the PercentageFeatureSet() function, which calculates the percentage of counts originating from a set of features
  * We use the set of all genes starting with MT- as a set of mitochondrial genes

```{r pre-processing , include=TRUE,echo=TRUE, message=F}
# The [[ operator can add columns to object metadata. This is a great place to stash QC stats
pbmc[["percent.mt"]] <- PercentageFeatureSet(pbmc, pattern = "^MT-")
# Show QC metrics for the first 5 cells
head(pbmc@meta.data, 5)
# Visualize QC metrics as a violin plot
VlnPlot(pbmc, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)

# FeatureScatter is typically used to visualize feature-feature relationships, but can be used
# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.

plot1 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(pbmc, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2

```
# Normalizing the data
After removing unwanted cells from the dataset, the next step is to normalize the data. By default, we employ a global-scaling normalization method “LogNormalize” that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. Normalized values are stored in pbmc[["RNA"]]@data.

# Identification of highly variable features (feature selection)
We next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). We and others have found that focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets.

Our procedure in Seurat is described in detail here, and improves on previous versions by directly modeling the mean-variance relationship inherent in single-cell data, and is implemented in the FindVariableFeatures() function. By default, we return 2,000 features per dataset. These will be used in downstream analysis, like PCA. 

```{r pre, include=TRUE,echo=TRUE, message=F}
#pbmc <- subset(pbmc, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)
pbmc <- NormalizeData(pbmc, normalization.method = "LogNormalize", scale.factor = 10000)

pbmc <- FindVariableFeatures(pbmc, selection.method = "vst", nfeatures = 2000)

# Identify the 10 most highly variable genes
top10 <- head(VariableFeatures(pbmc), 10)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(pbmc)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
plot1 + plot2

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(pbmc)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
plot1 + plot2

#Next, we apply a linear transformation (‘scaling’) that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The ScaleData() function:
# Shifts the expression of each gene, so that the mean expression across cells is 0
# Scales the expression of each gene, so that the variance across cells is 1
  # This step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate
# The results of this are stored in pbmc[["RNA"]]@scale.data
all.genes <- rownames(pbmc)
pbmc <- ScaleData(pbmc, features = all.genes)

# ‘regress out’ heterogeneity associated with (for example) cell cycle stage, or mitochondrial contamination.
pbmc <- ScaleData(pbmc, vars.to.regress = "percent.mt")   #vars.to.regress=variables to regress out

# PCA : Linear dimentionality reduction 
#the influence of each gene on a PC and the importance of each gene to the PC can be calculated as weights, these weights are called loadings and an array of loadings for a PC is called eigenvector.when we say PC1 is a linear combination of genes A and B, it means the linear line fitted to scatter plot of A and B in a way that it maximizes variation on the line or minimizes the SS distances of the datapoints from the line.the proportions of each gene to make the linear combination e.g. 0.7 A and 0.3 B, is "loading scores". The one unit long vector consisting of 0.7 units A and 0.3 units B (dividing the length of all three vectors by the length of the hypotenuse calculated by r^2 = a^2 + b^2)

pbmc <- RunPCA(pbmc, features = VariableFeatures(object = pbmc))
# Examine and visualize PCA results a few different ways
print(pbmc[["pca"]], dims = 1:5, nfeatures = 5)
VizDimLoadings(pbmc, dims = 1:2, reduction = "pca")
DimPlot(pbmc, reduction = "pca")

#In particular DimHeatmap() allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. Both cells and features are ordered according to their PCA scores. Setting cells to a number plots the ‘extreme’ cells on both ends of the spectrum, which dramatically speeds plotting for large datasets. Though clearly a supervised analysis, we find this to be a valuable tool for exploring correlated feature sets.

DimHeatmap(pbmc, dims = 1, cells = 500, balanced = TRUE)

```

# Cluster the cells
Seurat v3 applies a graph-based clustering approach, building upon initial strategies in (Macosko et al). Importantly, the distance metric which drives the clustering analysis (based on previously identified PCs) remains the same. However, our approach to partitioning the cellular distance matrix into clusters has dramatically improved.
Our approach was heavily inspired by recent manuscripts which applied graph-based clustering approaches to scRNA-seq data [SNN-Cliq, Xu and Su, Bioinformatics, 2015] and CyTOF data [PhenoGraph, Levine et al., Cell, 2015]. Briefly, these methods embed cells in a graph structure - for example a K-nearest neighbor (KNN) graph, with edges drawn between cells with similar feature expression patterns, and then attempt to partition this graph into highly interconnected ‘quasi-cliques’ or ‘communities’.

As in PhenoGraph, we first construct a KNN graph based on the euclidean distance in PCA space, and refine the edge weights between any two cells based on the shared overlap in their local neighborhoods (Jaccard similarity). This step is performed using the FindNeighbors() function, and takes as input the previously defined dimensionality of the dataset (first 10 PCs).

To cluster the cells, we next apply modularity optimization techniques such as the Louvain algorithm (default) or SLM [SLM, Blondel et al., Journal of Statistical Mechanics], to iteratively group cells together, with the goal of optimizing the standard modularity function. The FindClusters() function implements this procedure, and contains a resolution parameter that sets the ‘granularity’ of the downstream clustering, with increased values leading to a greater number of clusters. We find that setting this parameter between 0.4-1.2 typically returns good results for single-cell datasets of around 3K cells. Optimal resolution often increases for larger datasets. The clusters can be found using the Idents() function.



```{r pre, include=TRUE,echo=TRUE, message=F}
pbmc <- FindNeighbors(pbmc, dims = 1:10)
pbmc <- FindClusters(pbmc, resolution = 0.5)

# Look at cluster IDs of the first 5 cells
head(Idents(pbmc), 5)
```


# Run non-linear dimensional reduction (UMAP/tSNE)
Seurat offers several non-linear dimensional reduction techniques, such as tSNE and UMAP, to visualize and explore these datasets. The goal of these algorithms is to learn the underlying manifold of the data in order to place similar cells together in low-dimensional space. Cells within the graph-based clusters determined above should co-localize on these dimension reduction plots. As input to the UMAP and tSNE, we suggest using the same PCs as input to the clustering analysis.

```{r pre, include=TRUE,echo=TRUE, message=F}
# If you haven't installed UMAP, you can do so via reticulate::py_install(packages =
# 'umap-learn')
pbmc <- RunUMAP(pbmc, dims = 1:10)

# note that you can set `label = TRUE` or use the LabelClusters function to help label
# individual clusters
DimPlot(pbmc, reduction = "umap", label = TRUE)
#?? how add legends?
?DimPlot 

saveRDS(pbmc, file = "./pbmc_tutorial.rds", )


```

#Finding differentially expressed features (cluster biomarkers)

Seurat can help you find markers that define clusters via differential expression. By default, it identifies positive and negative markers of a single cluster (specified in ident.1), compared to all other cells. FindAllMarkers() automates this process for all clusters, but you can also test groups of clusters vs. each other, or against all cells.

ident.1	
Identity class to define markers for; pass an object of class phylo or 'clustertree' to find markers for a node in a cluster tree; passing 'clustertree' requires BuildClusterTree to have been run

ident.2	
A second identity class for comparison; if NULL, use all other cells for comparison; if an object of class phylo or 'clustertree' is passed to ident.1, must pass a node to find markers for

The min.pct argument requires a feature to be detected at a minimum percentage in either of the two groups of cells, and the thresh.test argument requires a feature to be differentially expressed (on average) by some amount between the two groups. You can set both of these to 0, but with a dramatic increase in time - since this will test a large number of features that are unlikely to be highly discriminatory. As another option to speed up these computations, max.cells.per.ident can be set. This will downsample each identity class to have no more cells than whatever this is set to. While there is generally going to be a loss in power, the speed increases can be significant and the most highly differentially expressed features will likely still rise to the top.

```{r pre, include=TRUE,echo=TRUE, message=F}
# find all markers of cluster 2
cluster2.markers <- FindMarkers(pbmc, ident.1 = 2, min.pct = 0.25)
head(cluster2.markers, n = 5)

# find all markers distinguishing cluster 5 from clusters 0 and 3
cluster5.markers <- FindMarkers(pbmc, ident.1 = 5, ident.2 = c(0, 3), min.pct = 0.25)
head(cluster5.markers, n = 5)

# find markers for every cluster compared to all remaining cells, report only the positive
# ones
pbmc.markers <- FindAllMarkers(pbmc, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)
pbmc.markers %>%
    group_by(cluster) %>%
    slice_max(n = 2, order_by = avg_log2FC)

#Seurat has several tests for differential expression which can be set with the test.use parameter (see our DE vignette for details). For example, the ROC test returns the ‘classification power’ for any individual marker (ranging from 0 - random, to 1 - perfect).

cluster0.markers <- FindMarkers(pbmc, ident.1 = 0, logfc.threshold = 0.25, test.use = "roc", only.pos = TRUE)
cluster0.markers

#We include several tools for visualizing marker expression. VlnPlot() (shows expression probability distributions across clusters), and FeaturePlot() (visualizes feature expression on a tSNE or PCA plot) are our most commonly used visualizations. We also suggest exploring RidgePlot(), CellScatter(), and DotPlot() as additional methods to view your dataset.

VlnPlot(pbmc, features = c("MS4A1", "CD79A"))

# you can plot raw counts as well
VlnPlot(pbmc, features = c("NKG7", "PF4"), slot = "counts", log = TRUE)

FeaturePlot(pbmc, features = c("MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP",
    "CD8A"))

pbmc.markers %>%
    group_by(cluster) %>%
    top_n(n = 10, wt = avg_log2FC) -> top10
DoHeatmap(pbmc, features = top10$gene) #+ NoLegend()


```


Assigning cell type identity to clusters
Fortunately in the case of this dataset, we can use canonical markers to easily match the unbiased clustering to known cell types:

Cluster ID	Markers	Cell Type
0	            IL7R, CCR7	Naive CD4+ T
1	            CD14, LYZ	CD14+ Mono
2	            IL7R, S100A4	Memory CD4+
3	            MS4A1	B
4	            CD8A	CD8+ T
5	            FCGR3A, MS4A7	FCGR3A+ Mono
6	            GNLY, NKG7	NK
7	            FCER1A, CST3	DC
8	            PPBP	Platelet


```{r pre, include=TRUE,echo=TRUE, message=F}
new.cluster.ids <- c("Naive CD4 T", "CD14+ Mono", "Memory CD4 T", "B", "CD8 T", "FCGR3A+ Mono",
    "NK", "DC", "Platelet")
names(new.cluster.ids) <- levels(pbmc)
pbmc <- RenameIdents(pbmc, new.cluster.ids)
DimPlot(pbmc, reduction = "umap", label = TRUE, pt.size = 0.5) + NoLegend()
```